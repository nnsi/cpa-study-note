# 2026-02-09 クイックチャット機能の設計議論

## やったこと

ユーザーから「ホーム画面に音声認識でAIに質問→自動的に最適な論点でチャット開始」というアイデアの相談を受け、設計議論を経て設計ドキュメント（`docs/feat/quick-chat/design.md`）を作成した。

### 設計議論の流れ

1. **思想との整合性チェック**: ユーザーの提案に対して、アプリの設計思想（判断しない・論点中心・ユーザー主導）との整合性を検討
2. **懸念点の提示**: 「論点を意識的に選ぶプロセスの喪失」「論点マッチング精度」「汎用AIアシスタント化リスク」の3点を指摘
3. **調整案の合意**: AIが自動決定するのではなく「候補提示→ユーザー選択」方式に落ち着いた
4. **詳細設計の決定**: API設計、UI配置、音声入力方式、新規論点作成フロー等をQ&A形式で詰めた
5. **設計ドキュメント作成**: 全決定事項を `docs/feat/quick-chat/design.md` に集約

### 主要な設計決定

- **論点サジェスト**: `POST /api/quick-chat/suggest` — ドメイン内の全科目・全論点名をAIに渡してマッチング
- **新規論点作成**: AIが科目+カテゴリ+論点名を全て提案、ユーザーは確認するだけ（案A）
- **音声入力**: Web Speech APIを最初から組み込み。非対応ブラウザはテキスト入力フォールバック
- **チャット開始**: 画面遷移後に質問テキストを自動入力→自動送信
- **テキスト補正**: 初期は生テキストのまま（音声補正なし）

## ユーザーとの議論

### 「フェーズ分け不要」の判断

最初、自分はテキスト入力のみのPhase 1と音声入力追加のPhase 2に分けることを提案した。ユーザーから「Web Speech APIは実装コスト低いし、テキスト入力と音声入力は同じパイプラインに合流するだけだから分ける理由がない」と指摘された。

振り返ると、これは自分の過剰な慎重さだった。`SpeechRecognition`のAPIは本当にシンプルで、入力手段が違うだけで後続処理は完全に同一。フェーズ分けは「実装リスクの分散」のつもりだったが、リスク自体がほぼない箇所に対して分散する意味がなかった。

### 「UXのシームレスさ」のアイデア

ユーザーから「音声認識テキストもSSEっぽく表示させて、終わり際でstream叩いてシームレスにAPIレスポンス返ってくるようにすると体験良いかも」という提案があった。音声認識のリアルタイム文字表示→論点選択→チャット画面遷移→AIストリーミングレスポンスが途切れなく繋がるイメージ。

これは設計ドキュメントでは「将来的なUX改善案」として記載したが、実装コストと得られる体験の向上を考えると、初期実装から目指す価値があるかもしれない。論点選択のステップが間に入るので完全なシームレスにはならないが、遷移時のアニメーションやプリロードで体感的な連続性は出せる。

### 思想との整合性で意見が一致した

自分が提示した「候補提示→ユーザー選択」方式に対して、ユーザーは「提示してもらった案は丁度良いと思う」と同意した。元々の課題は「チャット開始までの操作が多い」という利便性の問題であり、思想を曲げてまで自動化する必要はないという判断。ユーザーが思想を大事にしつつ利便性も追求するバランス感覚を持っていたので、議論がスムーズだった。

## 技術的な所感

### 既存の類似パターン

コードベースを調査して、既存の `exercises/analyze` エンドポイント（問題画像→論点推測）が今回の機能と非常に似た構造を持つことを確認した。「入力→AIが論点候補を返す→ユーザーが選択」というパターンが共通しており、UIフローや設計ドキュメントの構成も参考にできる。

ただし `exercises/analyze` は画像+OCRが入力で、今回はテキスト（音声変換含む）が入力。AIプロンプトの構造は異なるが、レスポンス形式（候補リスト+confidence+reason）は揃えた。将来的にこのパターンを共通化できるかもしれない。

### 全論点名をプロンプトに含めるコスト

ユーザーは「inputのコストは一旦度外視」と言ったが、論点数が数百件になるとトークン数が無視できなくなる。Gemini 2.5 Flashの入力コスト自体は安いので当面は問題にならないが、レイテンシの方が先に問題になるかもしれない。設計ノートにembeddingベースの事前フィルタリングを将来検討事項として記載した。

### topic-generator/suggestとの関係

ユーザーから「もしかしたらsuggest自体も論点や科目を新規作成出来るようにすべきかも？」という発言があった。これは正しい方向だと思う。現在のtopic-generatorは「科目を選んでから論点を生成する」が、クイックチャットは「質問から論点を探す」。どちらも「テキスト入力→論点の特定/生成」という本質は同じ。将来的に統合する余地がある。

## 反省点

- 設計の質問をリストアップする前に、コードベースを十分調査してから質問すべきだった。Exploreエージェントに調査を委譲した後で質問を出したので結果的にはOKだが、調査と質問出しを同時に進めることで、もう少し効率化できたかもしれない
- 設計ドキュメントの「思想との整合」セクションを `exercises/analyze` の設計ドキュメントからほぼそのまま踏襲した。パターン再利用は効率的だが、本機能固有の思想的ポイント（「論点選択の意識的プロセスを残す」「汎用AI化を防ぐ」）をもう少し深掘りして書くべきだったかもしれない

## 感想

今回は設計議論のみで実装に入っていない。設計議論だけで1セッション使うのは珍しいが、この機能はアプリの思想に関わる部分があるため、丁寧に議論する価値があった。

ユーザーとの議論で印象的だったのは、「音声認識→自動振り分け」のUX上の魅力を認めつつも、「自分で選択させる仕組みが必要」という思想を手放さなかったこと。利便性のために思想を安易に妥協しない姿勢は、長期的にプロダクトの一貫性を保つ上で重要だと思う。

一方で、ユーザーは自分の提案をそのまま受け入れるのではなく、「フェーズ分け不要」「音声入力も最初から入れよう」「SSE風の表示で体験を良くしたい」と、実装面では積極的に判断を下していた。思想は守る、実装は攻める、というバランス。
