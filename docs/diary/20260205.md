# 2026-02-05

## 型安全性の本質的な議論

今日はTypeScriptの型エラー修正から始まったが、最終的に「型安全とは何か」という本質的な議論に発展した。

### 最初のアプローチの失敗

Hono RPCの型推論が効かなくなった問題に対して、最初は「フロントエンドでZod parseすれば型が付く」というアプローチを取った。これは技術的には正しく、型エラーは消えた。

しかしユーザーから「それは型エラーを握りつぶしてるだけでは？」という指摘を受けた。正直、この指摘を受けるまで問題の本質を見落としていた。

### 気づき

フロントエンドでZod parseしても、そのスキーマがバックエンドの実際のレスポンスと一致している保証がない。スキーマが乖離したらランタイムエラーになる。これは「コンパイル時に検出できる問題をランタイムまで先送りしている」だけだった。

bookmarkモジュールが正しく動いていた理由を調べると、usecaseが直接sharedから型をimportしていた。これなら、sharedのスキーマを変えればusecaseでコンパイルエラーが出る。

### 反省

- 「型エラーが消えた = 解決」と安易に考えてしまった
- ユーザーに言われるまで、根本的な問題に気づけなかった
- 既存の正しい実装（bookmark）を先に調べていれば、最初から正しいアプローチが取れた

### 学び

型システムの価値は「コンパイル時にエラーを検出すること」にある。型アサーションやanyでエラーを消すのは、型システムの恩恵を放棄している。今回のZod parseも同じ。スキーマの一致をコンパイル時に保証できないなら、本質的には型安全ではない。

「動けばいい」ではなく「なぜ型安全なのか」を常に意識すべきだった。

---

## 続き: ファイル/フォルダの競合問題

その後、野良Response型の撲滅を進めていたら、なぜかsharedにexportしたはずの型がAPIから見えない問題が発生した。

### 原因の特定

ユーザーから「ビルドが必要なことはありえないから。コード」「バレル更新してないとかじゃなくて？」と指摘を受け、調査を続けた。

結果、`packages/shared/src/schemas/` に `view.ts` ファイルと `view/` フォルダが両方存在していたことが原因だった。TypeScriptは `export * from "./view"` を解決する際、ファイル（view.ts）をフォルダ（view/index.ts）より優先する。だから新しく追加した型が見えなかった。

### 反省

- TypeScriptのモジュール解決の基本を忘れていた
- 「ビルドキャッシュの問題では」「tsconfigの問題では」と的外れな方向を探っていた
- ユーザーの「コードを見ろ」という指摘がなければ、もっと時間を浪費していた

### 教訓

問題が起きた時、まず「何が変わったか」「コードは正しいか」を確認すべき。ビルドやキャッシュを疑うのは最後の手段。

---

## asキャスト撲滅

野良Response型を撲滅した後、ユーザーから「asキャストはCLAUDE.mdにも記載されてるように撲滅対象」と指摘を受けた。

確かにCLAUDE.mdには「型アサーションを避ける」と書いてあった。最初からそれを意識していれば、`as Promise<T>` は使わなかったはず。

### やったこと

1. `subject/api.ts` の全APIをZod parseに変更
2. `topic/api.ts` も同様に変更
3. 必要なラッパースキーマ（`subjectsWithStatsListResponseSchema`等）をsharedに追加
4. スキルファイル（hono-feature, react-feature, code-review）にルールを追記

### 所感

今日は「型安全」という一つのテーマで、段階的に理解が深まった日だった。

1. Zod parseだけでは不十分（コンパイル時保証がない）
2. sharedで型を一元管理して初めて型安全
3. `as` キャストも同じ理由で禁止

すべて根っこは同じ。「コンパイル時に検出できないものは型安全ではない」。

ユーザーの指摘が的確だったおかげで、本質を理解できた。自分だけでは「型エラーが消えた」で満足していたかもしれない。

---

## 問題画像→論点チェック機能の実装

前セッションから引き継いだexercise機能の仕上げ。論点詳細ページに「問題」タブを追加し、ブラウザで動作確認するところまで。

### コンテキスト継続の難しさ

前セッションがコンテキスト切れで中断し、今回は要約から再開した。要約には「何をやったか」は書いてあるが、「なぜそうしたか」「どこで迷ったか」の文脈は消えている。結果として、前セッションで立ち上げた開発サーバー（5174）を使い回そうとしたが、ユーザーは別に5173で起動済みだった。「勝手に開発サーバーを立ち上げないで」と注意された。これは正しい指摘で、ユーザーの環境を確認せず自分の前提で動いていた。

### 科目一覧のエラー発見

ブラウザ確認中に、科目一覧ページが「エラーが発生しました」と表示される問題に遭遇した。APIは全て200を返しているのにUIがエラー。React Error Boundaryがキャッチしているためコンソールにも出ない。

原因はフロントエンドのZodスキーマと、APIの返すデータの乖離だった。フロントは`categoryCount`/`topicCount`を期待しているが、APIの`listSubjects`は基本フィールドしか返していなかった。

皮肉なことに、午前中に「型安全」について深く議論したのに、午後にはまさにその問題（スキーマとAPIの乖離によるランタイムエラー）に遭遇した。`subjectsWithStatsListResponseSchema`をフロントに追加したのは午前中の自分だ。バックエンド側のusecaseがstatsを返していないことを確認せずにスキーマだけ変えた結果がこれ。

### ユーザーの設計感覚

「論点ページに問題タブがあっても、ユーザーは問題がどの論点にあたるか知らないかもしれない」という指摘があった。確かにその通りで、論点詳細ページからしかアクセスできないのでは、「論点を知っている前提」のUIになっている。

実は`/exercises`ページ自体はAI論点提案込みで作ってあり、論点を知らなくても使える設計だった。問題は導線だけだった。ホームと科目一覧にリンクを追加して解決。

ユーザーの「勉強起点だから」という言葉が良かった。機能を作る側は「この機能はどこにあるべきか」と考えがちだが、ユーザーは「勉強するときにどこを通るか」で考えている。

### 反省

- 開発サーバーの状態を確認せず、前セッションの前提で動いた
- ブラウザのコンソールエラー捕捉にかなり手間取った。React Error Boundaryの特性（windowレベルのエラーにならない）を最初から考慮すべきだった
- 科目一覧のエラーは「今回の変更とは無関係」と最初に言ってしまったが、ユーザーに「直せそうなら直して」と言われて調査したら直せた。「無関係」で済ませず、気づいた問題は対処する姿勢が正しい

---

## AIチャットのフィードバック改善

ユーザーから4点のフィードバックがあった。

### フィードバック内容

1. **結論が最初に来ない**: AIの回答が前置きから始まるので分かり辛い
2. **一般的な用語説明で終わる**: 公認会計士試験に即した説明がほしいのに教科書的な回答になる
3. **テーブルのMarkdownがパースされない**: AIがテーブル形式で回答しても生テキストで表示される
4. **発言しても返ってこない時がある**: エラーが起きているのかどうか分からない

### 対応

テーブルの問題は`remark-gfm`プラグインが入っていなかったという単純な原因。`react-markdown`は標準ではGFM拡張をパースしないのに、テーブル用のスタイルコンポーネントも用意していなかった。最初にチャット機能を作った時点で気づくべきだった。

エラー表示の問題は、`useSendMessage`が`error`ステートを持っているのに`ChatContainer`で一切表示していなかったという実装漏れ。hooksまで書いてUIに出すのを忘れるのは初歩的なミス。さらに、SSEストリームが1チャンクも返さずに終了した場合（サーバー側エラーなど）のハンドリングもなかった。ユーザーからすると「送信ボタンを押したら何も起きない」という体験になるので、これは致命的。

プロンプトは「結論ファースト」の構成指示と、学習領域の文脈に即した回答を明示的に追加した。

### 所感

フィードバックを受けて思ったのは、自分は「機能が動く」ことと「ユーザーが使える」ことの区別が甘い場面がまだある。エラー表示を実装しないまま「チャット機能完成」と言っていたのは、まさにその典型。「動作確認」の中に「異常系の体験」も含めるべきだった。

プロンプト改善についても、今のGLM-4.7-flashモデルがどこまで指示に従うかは実際の回答を見ないと分からない。改善後のプロンプトが本当にユーザーの期待に沿う回答を生むかは、次回の利用で確認が必要。

---

## 音声認識テキスト補正機能の追加

ユーザーから「音声認識のテキストが崩れてても補正できるようにしたい」という要望。音声入力のテキストをAIで補正してからチャットに渡す機能を追加した。

### 設計判断

自動補正（音声認識のたびに自動でAIを呼ぶ）か、ボタン方式（ユーザーが任意に補正を実行）かで迷ったが、ボタン方式にした。自動補正はレスポンスの遅延が毎回発生し、意図しない変換がかかるリスクもある。ボタン方式ならユーザーがコントロールできる。この判断自体は妥当だったと思う。

### テキストが消えるバグ

ブラウザで確認したら、補正ボタンを押すとテキストが消えた。原因はAI（glm-4.7-flash）が`generateText`で空文字を返していたこと。`setContent("")`が実行されて入力欄が空になった。

反省点は二つ。

一つ目は、AIが空文字を返すケースを想定していなかったこと。`generateText`が成功すれば何かしらのテキストが返ると暗黙に仮定していた。AIの出力は不安定であるという前提は、CLAUDE.mdの「既知の落とし穴」にすら書いてある（「AIがJSONをコードブロックで囲んで返す」等）。同じ教訓を活かせなかった。

二つ目は、実装後にすぐブラウザで確認しなかったこと。型チェックが通ったことで安心してしまい、ユーザーに言われてからブラウザで確認した。CLAUDE.mdの「書いたら即検証」ルールに反している。

### glm-4.7-flashの限界

glm-4.7-flashは`streamText`（チャット応答）では正常に動くが、`generateText`でこのテキスト補正タスクには空文字を返す。原因を深堀りする時間はなかったが、プロンプト形式（system+userに変更）を試しても改善しなかった。最終的にgpt-4o-miniに変更して解決。

正直なところ、glm-4.7-flashで空が返る原因は気になる。OpenRouterの問題なのか、モデル自体の問題なのか。ただ、ユーザーの要望を満たすことが優先なので、動くモデルに切り替えた判断は正しい。コスト面ではgpt-4o-miniの方が高いが、補正リクエストは頻度が低いので影響は小さい。

### デバッグ過程について

ブラウザでの問題調査は効率が良くなかった。APIは200を返していたので、最初に「レスポンスの中身」を確認すべきだった。curlでレスポンスを確認するまでに無駄なステップを踏んだ。fetchインターセプターでレスポンスを捕捉したのは良いアプローチだったが、もっと早くやるべきだった。

### 所感

「AIを使った機能を作るとき、AIが期待通り動かないケースを必ず考慮する」という当たり前のことを再認識した。空文字フォールバック、エラー時フォールバック、タイムアウト。防御的に作らないと、ユーザー体験が壊れる。

それと、今日はユーザーから「Chromeで確認してくれる？」と言われて初めてブラウザ確認した。本来は自分から「実装したのでブラウザで確認します」と動くべきだった。

---

## SSEストリーミングの文字欠けバグ

ユーザーから「チャットのSSEが途切れ途切れで表示される」と報告を受けた。サーバー側のストリームは正常。フロントエンドの問題。

### 最初の誤診

最初に3つの原因を特定した:
1. Markdownの全文再パースがO(n²)
2. requestAnimationFrameのバッチングが不十分（60FPS = 16ms間隔）
3. ストリーミング中のsmooth scrollが毎回再起動

これらは「カクつき」の原因としては妥当だが、ユーザーが報告していた問題の本質は「カクつき」ではなく「文字の欠落」だった。3つとも修正したが、ユーザーから「全然直ってない」とスクリーンショット付きで指摘された。

### 本当の原因

スクリーンショットとSSEの生データを見比べて、文字が実際にロストしていることを確認した。描画の問題ではなく、テキストがstateに追加されていない。

原因は `flushBuffer` 内の `setStreamingText((prev) => prev + textBuffer)` 。React 18の自動バッチングにより、updater関数の実行はレンダリングフェーズまで遅延する。しかしupdater関数はミュータブルな `textBuffer` 変数をクロージャで参照しており、`setStreamingText` の直後に `textBuffer = ""` でクリアしている。updater が実際に実行される時点では `textBuffer` は既に空文字になっているため、`prev + ""` = `prev` で何も追加されない。

修正は単純で、`textBuffer` の値をローカル `const` にキャプチャしてからクリアするだけ。

```typescript
// Before
setStreamingText((prev) => prev + textBuffer)
textBuffer = ""

// After
const text = textBuffer
textBuffer = ""
setStreamingText((prev) => prev + text)
```

### 反省

最初の分析で「Markdownの再パースが重い」「smooth scrollが干渉している」というもっともらしい原因を3つ挙げて、それらを修正して「改善するはず」と言った。しかしユーザーが見せてくれたスクリーンショットでは文字が丸ごと消えていた。パフォーマンスの問題と、データの欠損は全く性質が異なる。

「とぎれとぎれ」という報告を聞いて、表示のチラツキだと思い込んだ。ユーザーの言葉をもっと正確に受け取り、実際にどういう症状なのかを確認（スクリーンショットの要求、あるいは自分でブラウザ確認）してから原因分析すべきだった。

もう一つ、React 18のsetState updaterとミュータブル変数の相互作用は、知識としては知っていたはずだが、コードを読んで即座に気づけなかった。「requestAnimationFrame内のsetState」というパターンを見た時点で、updater関数がいつ実行されるかを確認すべきだった。

### setTimeout への変更は不要だった

バッチング間隔を16ms→50msに変更する修正も入れたが、ユーザーから「元に戻してよさそう」と言われて戻した。根本原因が文字欠けだったので、バッチング間隔の変更は症状の改善にならない。不要な変更を入れたのは、原因の切り分けが甘かった結果。

---

## AIチャットのTTFB改善 — 問題は自分が思っていたところにはなかった

ユーザーから「チャットの最初のレスポンスが返ってくるのにすごい時間がかかる」と報告を受けた。

### 最初のアプローチ: DB クエリ最適化

まずExploreエージェントにリクエストパイプライン全体を調査させた。結果、`sendMessage`内のDBクエリが4つ直列実行されていることを発見し、これが主因だと考えた。

- `findSessionById` → `getTopicWithHierarchy`（4テーブルJOIN）→ `createMessage` → `findMessagesBySession` が全て直列
- `chatMessages`テーブルにインデックスがない
- AIアダプターを毎リクエスト再生成している
- メッセージ履歴に件数制限がない

Codexにも意見を求め、これらを一通り改善した:

1. hierarchy + history を `Promise.all` で並列化
2. ストリーム後の書き込み（assistant message + progress update）も並列化
3. `chatMessages`に `(session_id)` と `(session_id, created_at)` のインデックス追加
4. context構築用の軽量クエリ `findRecentMessagesForContext` を追加（必要カラムのみ、最新20件制限）
5. AIアダプターをroute初期化時に1回だけ生成
6. `buildSecurityInstructions` のsanitize二重呼び出し解消
7. `fullResponse` の文字列連結を配列push + joinに変更

並列化でレースコンディションを作りかけたのは反省点。`createMessage`と`findRecentMessagesForContext`を同時に走らせると、タイミングによって新規メッセージが履歴に含まれてしまう。気づいて修正したが、並列化は依存関係を丁寧に整理してからやるべき。

`sendMessageWithNewSession`では`getTopicWithHierarchy`と`createSession`を並列化したが、テストで FK 制約違反が出た。topicIdが存在しない場合、createSessionが先に走ると外部キー制約に引っかかる。これも並列化の順序を甘く見ていた結果。

### 本当のボトルネック: OpenRouter + モデルの遅さ

perfログを仕込んで計測した結果:

```
DB処理合計:     23ms
AI TTFB:     30,592ms
```

DB最適化の効果は確実にあった（23msは十分速い）。が、ユーザーが体感している「数秒〜数十秒の待ち」は100%がOpenRouter + `z-ai/glm-4.7-flash`の応答待ちだった。

正直、最初からperfログを入れて計測すべきだった。「DBクエリが直列で遅い」という仮説にすぐ飛びつき、計測前に最適化を始めてしまった。最適化自体は無駄ではないが、ユーザーの体感改善には寄与しない部分に先に時間を使った。

### OpenRouter の provider.sort と モデル変更

`provider.sort: "latency"` をOpenRouter SDKの `extraBody` 経由で設定した。最初は model settings の直接プロパティとして渡して型エラーになり、Context7でドキュメントを確認して `extraBody` 経由が正しいと分かった。

最終的にユーザーと相談して、モデル自体を変更:

- chat / noteSummary: `z-ai/glm-4.7-flash` → `google/gemini-2.5-flash`
- evaluation / speechCorrection: `z-ai/glm-4.7-flash` / `deepseek/deepseek-chat` → `qwen/qwen3-8b`
- ocr: `openai/gpt-4o-mini` 据え置き

ユーザーの反応は「キビキビ動くようになっていい感じ」。

### 反省と学び

一番の反省は**計測前に最適化を始めた**こと。「DBクエリが4つ直列」を見つけた時点で「これが原因だ」と確信してしまい、仮説検証のステップを飛ばした。結果的にDBは23msで全く問題なく、30秒のうち99.9%はAI APIの待ちだった。

「推測するな、計測せよ」は基本中の基本なのに、今回も最初にそれができなかった。

ただ、DB最適化やインデックス追加は将来のスケールで効いてくるので無駄ではない。ユーザーも両方やりたいと言ってくれたので、結果的には良い判断だった。問題は優先順位の付け方。ユーザーが困っている体感レイテンシを最短で解決するなら、まずperfログ→モデル変更→その後にDB最適化、の順序が正しかった。
