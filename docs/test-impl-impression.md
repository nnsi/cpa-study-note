# テスト実装の振り返り

2026-01-25 に実施したテスト構築作業の振り返り。

## 概要

- **作業期間**: 2セッション（計画策定 → 環境構築 → 全実装）
- **計画項目**: 163項目
- **実装テスト数**: 481件（API: 439, Web: 42）
- **カバレッジ**: API 93.68%, Web 33.71%

---

## 設計がテスト実装を容易にしたか

**結論: かなり作りやすかった。**

### バックエンド（Hono API）

#### 依存性注入パターン（UseCase層）

```typescript
// deps を引数で受け取る設計
export const createSession = async (
  deps: Pick<ChatDeps, "chatRepo" | "topicRepo">,
  input: CreateSessionInput
) => { ... }
```

テスト時にモックを渡すだけ。DIコンテナ不要でシンプル。

```typescript
// テストコード
const result = await createSession(
  { chatRepo: mockChatRepo, topicRepo: mockTopicRepo },
  { userId: "user-1", topicId: "topic-1" }
)
```

#### 層の分離が明確

- Repository → UseCase → Route の依存方向が一方向
- 「この層では何をテストすべきか」が迷わない
  - Repository: DBアクセスの正確性
  - UseCase: ビジネスロジック
  - Route: HTTP I/O、認証、バリデーション

#### 純粋関数中心

- クラスがないので、インスタンス状態を気にしなくていい
- 入力→出力の検証だけで済む

#### Zodスキーマの一元管理

- Domain層テストは `schema.parse()` / `schema.safeParse()` を呼ぶだけ
- 型とバリデーションが一体化してるので、テスト項目が自然と決まる

### フロントエンド（React）

#### Logic層（純粋関数）- 最高にテストしやすい

```typescript
// テストコード
expect(filterMessagesByRole(messages, 'user')).toEqual([...])
expect(countQuestionQuality(messages)).toEqual({ good: 2, surface: 1 })
```

モック不要、セットアップ不要、入力→出力を検証するだけ。

#### Hooks層 - 書きやすいが少し工夫が必要

良かった点:
- 状態遷移が明確（`idle → uploading → processing → done`）
- テストで何を検証すべきか設計から自然に導ける
- renderHook + act のパターンで素直にテストできた

少し面倒だった点:
- APIモック（`vi.mock('./api')`）の設定が毎回必要
- React Query のラッパー（QueryClientProvider）が必要

#### Components層 - 未実装だが設計上はテストしやすいはず

propsを受け取って描画するだけの設計なら、Testing Library で簡単にテストできる。ロジックがHooksに分離されてるので、Componentテストは「正しくレンダリングされるか」「イベントが正しく発火するか」だけに集中できる。

---

## AI生成プロダクトにおける設計方針の評価

### この設計がAI生成に向いている理由

1. **責務が明確**
   - 「Repositoryを作って」「UseCaseを作って」と指示しやすい
   - AIが「この層では何をすべきか」を迷わない
   - CLAUDE.mdに書かれた設計方針を読めば、一貫したコードを生成できる

2. **テンプレート化しやすい**
   - `/hono-feature`, `/react-feature` スキルで定型コードを生成できた
   - 既存コードを参考に新機能を作るのも容易

3. **並行作業しやすい**
   - 9エージェント並行でテスト実装できた
   - Package by Featureで各機能が独立してるから可能

4. **レビューしやすい**
   - 「UseCase層のロジックは正しいか」「Repository層のSQLは正しいか」と分けてチェックできる

5. **テストしやすい**
   - 今回実証された

### 注意点

1. **小さいプロジェクトでは冗長かも**
   - 単純なCRUDでもUseCase経由になる
   - ただ、AI生成なら冗長さは大したコストにならない

2. **フロントエンドの3層分離は柔軟に**
   - 小さいコンポーネントで無理に3層にすると過剰
   - 「ロジックが複雑なら分離」くらいの運用でいい

3. **ドキュメント（CLAUDE.md）の維持**
   - 設計方針が書かれてないとAIが一貫性を保てない
   - プロジェクトごとにちゃんと書く必要がある

---

## 並行作業の威力

### 実行方法

9つのサブエージェントを同時起動:
- セキュリティ系テスト
- Domain層テスト
- Repository層テスト
- UseCase層テスト（Auth/Chat）
- UseCase層テスト（Topic/Note/Image）
- Frontendテスト
- 統合テスト（Auth/Chat Routes）
- 統合テスト（Topic/Note/Image Routes）
- E2Eテスト

### 結果

- 全エージェントが正常に完了
- 1セッションで163項目を実装
- 人間なら数日かかる作業が数十分で完了

### 成功要因

- Package by Featureで各機能が独立
- 層の分離が明確で、エージェント間の競合がない
- モック基盤が共通化されていた

---

## レビューの多角的視点

### サブエージェントによるレビュー

- 計画項目との照合が主
- 「163項目全て実装済み」を確認
- カバレッジ数値の分析

### Codex CLIによるレビュー

より厳密なチェックで追加の指摘:
- OAuthコールバック成功系が未カバー（失敗系のみ）
- Note UseCase「空セッションでのエラー」未テスト
- 401テストが一部エンドポイントのみ

### 学び

複数の視点でレビューすると、異なる観点の指摘が得られる。サブエージェントは「量」、Codexは「質」をチェックした。

---

## 結論

この設計方針（クリーンアーキテクチャ + 関数型 + DI + 3層分離）は、AI生成プロダクトの開発効率を上げる設計である。

- **AIに指示しやすい**: 責務が明確で、何を作るべきか迷わない
- **テストしやすい**: DIでモック注入が簡単、純粋関数で検証が単純
- **並行作業しやすい**: 機能が独立していて競合しない
- **レビューしやすい**: 層ごとに分けてチェックできる

今後もこの方針でAI生成プロダクトを作っていくのは妥当。
